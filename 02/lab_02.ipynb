{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Lab 02\n",
    "## Adrien Giget, Tanguy Malandain, Denis Stojiljkovic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "import numpy as np\n",
    "from typing import Dict, Any, TypeVar\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/Tanguy/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 33.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ds.load_dataset(\"imdb\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset (3 points)\n",
    "\n",
    "   * How many splits does the dataset has? (1 point)\n",
    "   * How big are these splits? (1 point)\n",
    "   * What is the proportion of each class on the supervised splits? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a 3 sets de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 25000, 'test': 25000, 'unsupervised': 50000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le set \"train\" à 25 000 données, ainsi que le set \"test\".\n",
    "Le set \"unsupervised\" en a 50 000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève le dataset unsupervised qui ne sera plus utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"unsupervised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Tanguy\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-00a580d4a08599a8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Tanguy\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-6fdcb91b1f3cc61f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_positive = data.filter(lambda x: x[\"label\"] == 1)\n",
    "nb_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Tanguy\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-c3579795bcf1a351.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Tanguy\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-20cc5fc08326d82a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_negative = data.filter(lambda x: x[\"label\"] == 0)\n",
    "nb_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les deux sets supervisés, nous avons la moitié des données labellisée positive et l'autre négative, donc 12500 de chaque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier (13 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2 points) Take a look at the data and create an adapted preprocessing function with at least:\n",
    "\n",
    "  * Lower case the text.\n",
    "  * Replace punctuations with spaces (you can use from string import punctuation to ease your work). Think that maybe not all punctuations should be removed or replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lower(comment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    :param comment: Actual customer comment as a dict { str : any }\n",
    "    :return: a modified dict { str : any }\n",
    "    \"\"\"\n",
    "    comment[\"text\"] = comment[\"text\"].lower()\n",
    "    return comment\n",
    "\n",
    "adapted_data = data.map(get_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who are these \"they\"- the actors? the filmmakers? certainly couldn\\'t be the audience- this is among the most air-puffed productions in existence. it\\'s the kind of movie that looks like it was a lot of fun to shoot\\x97 too much fun, nobody is getting any actual work done, and that almost always makes for a movie that\\'s no fun to watch.<br /><br />ritter dons glasses so as to hammer home his character\\'s status as a sort of doppleganger of the bespectacled bogdanovich; the scenes with the breezy ms. stratten are sweet, but have an embarrassing, look-guys-i\\'m-dating-the-prom-queen feel to them. ben gazzara sports his usual cat\\'s-got-canary grin in a futile attempt to elevate the meager plot, which requires him to pursue audrey hepburn with all the interest of a narcoleptic at an insomnia clinic. in the meantime, the budding couple\\'s respective children (nepotism alert: bogdanovich\\'s daughters) spew cute and pick up some fairly disturbing pointers on \\'love\\' while observing their parents. (ms. hepburn, drawing on her dignity, manages to rise above the proceedings- but she has the monumental challenge of playing herself, ostensibly.) everybody looks great, but so what? it\\'s a movie and we can expect that much, if that\\'s what you\\'re looking for you\\'d be better off picking up a copy of vogue.<br /><br />oh- and it has to be mentioned that colleen camp thoroughly annoys, even apart from her singing, which, while competent, is wholly unconvincing... the country and western numbers are woefully mismatched with the standards on the soundtrack. surely this is not what gershwin (who wrote the song from which the movie\\'s title is derived) had in mind; his stage musicals of the 20\\'s may have been slight, but at least they were long on charm. \"they all laughed\" tries to coast on its good intentions, but nobody- least of all peter bogdanovich - has the good sense to put on the brakes.<br /><br />due in no small part to the tragic death of dorothy stratten, this movie has a special place in the heart of mr. bogdanovich- he even bought it back from its producers, then distributed it on his own and went bankrupt when it didn\\'t prove popular. his rise and fall is among the more sympathetic and tragic of hollywood stories, so there\\'s no joy in criticizing the film... there _is_ real emotional investment in ms. stratten\\'s scenes. but \"laughed\" is a faint echo of \"the last picture show\", \"paper moon\" or \"what\\'s up, doc\"- following \"daisy miller\" and \"at long last love\", it was a thundering confirmation of the phase from which p.b. has never emerged.<br /><br />all in all, though, the movie is harmless, only a waste of rental. i want to watch people having a good time, i\\'ll go to the park on a sunny day. for filmic expressions of joy and love, i\\'ll stick to ernest lubitsch and jaques demy...'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapted_data[\"train\"][\"text\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_angle_brackets(comment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove everything inside < > characters (usually <br > from scraping)\n",
    "    :param comment: Actual customer comment as a dict { str : any }\n",
    "    :return: a modified dict { str : any }\n",
    "    \"\"\"\n",
    "    comment[\"text\"] = re.sub(f'<.*?>', '', comment[\"text\"])\n",
    "    return comment\n",
    "\n",
    "def remove_unknown_unicode(comment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove all non-usual unicode characters (as —)\n",
    "    :param comment: Actual customer comment as a dict { str : any }\n",
    "    :return: a modified dict { str : any }\n",
    "    \"\"\"\n",
    "    comment[\"text\"] = re.sub(r\"(\\\\x\\S+)\", '', repr(comment[\"text\"])[1:-1])\n",
    "    return comment\n",
    "\n",
    "def remove_backslash(comment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove all backslash (usually as \"it\\'s\")\n",
    "    :param comment: Actual customer comment as a dict { str : any }\n",
    "    :return: a modified dict { str : any }\n",
    "    \"\"\"\n",
    "    comment[\"text\"] = re.sub(r\"\\\\\", '', comment[\"text\"])\n",
    "    return comment\n",
    "\n",
    "def remove_ponctuation(comment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove non-wanted ponctuations as _\\(\\)\\\".,`:;* that don't add much syntactical meaning.\n",
    "    :param comment: Actual customer comment as a dict { str : any }\n",
    "    :return: a modified dict { str : any }\n",
    "    \"\"\"\n",
    "    comment[\"text\"] = re.sub(r\"[_\\(\\)\\\".,`:;*]\", ' ', comment[\"text\"])\n",
    "    comment[\"text\"] = re.sub(r\"[(!?)]\", r\" \\g<0> \", comment[\"text\"]) # We want to keep ? and ! in the split\n",
    "    return comment\n",
    "\n",
    "def remove_all(comment: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply all above\n",
    "    :param comment: Actual customer comment as a dict { str : any }\n",
    "    :return: a modified dict { str : any }\n",
    "    \"\"\"\n",
    "    return remove_ponctuation(remove_backslash(remove_unknown_unicode(remove_angle_brackets(get_lower(comment)))))\n",
    "\n",
    "adapted_data = data.map(remove_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"who are these  they - the actors ?  the filmmakers ?  certainly couldn't be the audience- this is among the most air-puffed productions in existence  it's the kind of movie that looks like it was a lot of fun to shoot too much fun  nobody is getting any actual work done  and that almost always makes for a movie that's no fun to watch ritter dons glasses so as to hammer home his character's status as a sort of doppleganger of the bespectacled bogdanovich  the scenes with the breezy ms  stratten are sweet  but have an embarrassing  look-guys-i'm-dating-the-prom-queen feel to them  ben gazzara sports his usual cat's-got-canary grin in a futile attempt to elevate the meager plot  which requires him to pursue audrey hepburn with all the interest of a narcoleptic at an insomnia clinic  in the meantime  the budding couple's respective children  nepotism alert  bogdanovich's daughters  spew cute and pick up some fairly disturbing pointers on 'love' while observing their parents   ms  hepburn  drawing on her dignity  manages to rise above the proceedings- but she has the monumental challenge of playing herself  ostensibly   everybody looks great  but so what ?  it's a movie and we can expect that much  if that's what you're looking for you'd be better off picking up a copy of vogue oh- and it has to be mentioned that colleen camp thoroughly annoys  even apart from her singing  which  while competent  is wholly unconvincing    the country and western numbers are woefully mismatched with the standards on the soundtrack  surely this is not what gershwin  who wrote the song from which the movie's title is derived  had in mind  his stage musicals of the 20's may have been slight  but at least they were long on charm   they all laughed  tries to coast on its good intentions  but nobody- least of all peter bogdanovich - has the good sense to put on the brakes due in no small part to the tragic death of dorothy stratten  this movie has a special place in the heart of mr  bogdanovich- he even bought it back from its producers  then distributed it on his own and went bankrupt when it didn't prove popular  his rise and fall is among the more sympathetic and tragic of hollywood stories  so there's no joy in criticizing the film    there  is  real emotional investment in ms  stratten's scenes  but  laughed  is a faint echo of  the last picture show    paper moon  or  what's up  doc - following  daisy miller  and  at long last love   it was a thundering confirmation of the phase from which p b  has never emerged all in all  though  the movie is harmless  only a waste of rental  i want to watch people having a good time  i'll go to the park on a sunny day  for filmic expressions of joy and love  i'll stick to ernest lubitsch and jaques demy   \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapted_data[\"train\"][\"text\"][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (4 points) Implement your own naive Bayes classifier from scratch. The pseudocode can be found in the slides or the book reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomClass = TypeVar('CustomClass')\n",
    "\n",
    "class CustomNaivesBayesClassifier:\n",
    "    \"\"\"\n",
    "    Our custom Naive Bayes Classifier from Speech and Language Processing (3rd ed. draft)\n",
    "    by Dan Jurafsky and James H. Martin\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._logPrior: Dict[CustomClass, float] = {}\n",
    "        self._logLikelihoods: Dict[str, Dict[CustomClass, float]] = {}\n",
    "        self._classes: list[CustomClass] = []\n",
    "        self.__vocabulary: set[str] = set()\n",
    "\n",
    "\n",
    "    def train_naive_bayes(self, data: list[str], label: list[CustomClass]) -> None:\n",
    "        \"\"\"\n",
    "        Implementation of the training algorithm\n",
    "        :param data: All comments made by customers\n",
    "        :param label: The associated label\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        n_doc = len(data)\n",
    "        self._classes = np.unique(label)\n",
    "        self.__vocabulary =  set(word for text in data for word in text.split())\n",
    "        self._logLikelihoods = {word: {} for word in self.__vocabulary}\n",
    "\n",
    "        for classe in self._classes:\n",
    "            n_c = len([index for index in range(len(data)) if label[index] == classe])\n",
    "            self._logPrior[classe] = np.log(n_c / n_doc)\n",
    "            big_doc =  [word for index in range(len(data)) if label[index] == classe for word in data[index].split()]\n",
    "            denominator = len(big_doc) + len(self.__vocabulary)\n",
    "            counter = Counter(big_doc)\n",
    "\n",
    "            for word in self.__vocabulary:\n",
    "                self._logLikelihoods[word][classe] = np.log((counter[word] + 1) / denominator)\n",
    "\n",
    "\n",
    "    def test_naive_bayes(self, data: list[str]) -> list[CustomClass]:\n",
    "        \"\"\"\n",
    "        The test of train_naive_bayes()\n",
    "        :param data: Testing non labeled data\n",
    "        :return: A list of class\n",
    "        \"\"\"\n",
    "        res = []\n",
    "\n",
    "        for text in data:\n",
    "            sum_c = self._logPrior.copy()\n",
    "            for word in text.split():\n",
    "                if word in self.__vocabulary:\n",
    "                    for classe in self._classes:\n",
    "                        sum_c[classe] += self._logLikelihoods[word][classe]\n",
    "            res.append(max(sum_c, key=sum_c.get))\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def get_10_highest_log(self) -> Dict[CustomClass, list[str]]:\n",
    "        \"\"\"\n",
    "        Get the 10 highest likehood in each class\n",
    "        :return: A dict of class containing list of str\n",
    "        \"\"\"\n",
    "        res = {}\n",
    "        for classe in self._classes:\n",
    "            logprior_c = {key: value[classe] for key, value in self._logLikelihoods.items()}\n",
    "            res[classe] = sorted(logprior_c, key=logprior_c.get, reverse=True)[:10]\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = CustomNaivesBayesClassifier()\n",
    "custom.train_naive_bayes(adapted_data[\"train\"][\"text\"], adapted_data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bayes_results = custom.test_naive_bayes(adapted_data[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3 points) Implement a naive Bayes classifier using scikit-learn.\n",
    "\n",
    "    * Use a scikit-learn Pipeline with a CountVectorizer and MultinomialNB classifier. You can use other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(adapted_data[\"train\"][\"text\"], adapted_data[\"train\"][\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_results = pipeline.predict(adapted_data[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) Report the accuracy on both training and test set, for both your implementation and the scikit-learn one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_bayes(predicted_data: list[Any], real_data: list[Any]) -> float:\n",
    "    return len([index for index in range(len(predicted_data)) if real_data[index] == predicted_data[index]]) / len(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9078"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_train_data = CustomNaivesBayesClassifier()\n",
    "custom_train_data.train_naive_bayes(adapted_data[\"train\"][\"text\"], adapted_data[\"train\"][\"label\"])\n",
    "custom_train_data_results = custom_train_data.test_naive_bayes(adapted_data[\"train\"][\"text\"])\n",
    "accuracy_bayes(custom_train_data_results, adapted_data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_train_data = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_train_data.fit(adapted_data[\"train\"][\"text\"], adapted_data[\"train\"][\"label\"])\n",
    "sklearn_train_data = pipeline.predict(adapted_data[\"train\"][\"text\"])\n",
    "accuracy_bayes(sklearn_train_data, adapted_data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81592"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_label = adapted_data[\"test\"][\"label\"]\n",
    "\n",
    "accuracy_bayes(custom_bayes_results, data_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81432"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(sklearn_results, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.33 s\n",
      "Wall time: 8.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "custom.train_naive_bayes(adapted_data[\"train\"][\"text\"], adapted_data[\"train\"][\"label\"])\n",
    "custom.test_naive_bayes(adapted_data[\"test\"][\"text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.16 s\n",
      "Wall time: 6.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline.fit(adapted_data[\"train\"][\"text\"], adapted_data[\"train\"][\"label\"])\n",
    "pipeline.predict(adapted_data[\"test\"][\"text\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après la documentation, la bibliothèque scikit-learn utilise quelques méthodes en plus d'un simple algorithme de Bayes naïf.\n",
    "Notamment un preprocessing des données : tokénisation du text et suppression des \"stops words\".\n",
    "Et une vectorisation du corpus de mots - en transformant donc chaque mot par un nombre - pour faciliter les calculs.\n",
    "\n",
    "\n",
    "Mais bien que plus lente, on remarque que notre version est plus performante en termes de précision de ses prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) Why is accuracy a sufficient measure of evaluation here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision de prédiction est une bonne mesure d'évaluation dans notre cas notre set de données de test est déjà labellisé.\n",
    "Nous sommes donc certains de l'efficacité de l'implémentation avec la seule mesure de la précision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) Using one of the implementation, take at least 2 wrongly classified example from the test set and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4642"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predicted_data = [data[\"test\"][index] for index in range(len(sklearn_results)) if data_test_label[index] != sklearn_results[index]]\n",
    "len(wrong_predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Ray is interesting in parts, and technically it's very well made, but Ray is often sluggish, and forgets some important details about Ray's life. All the movie shows us is parts where he's in his prime, and most successful, which is good, it's just I wanted to see some bits about his older life too. Jamie Fox mimics Ray Charles to a t, at times it's absolutely uncanny. I have to say, Jamie is the reason I got through this movie. The 1st half is a lot better then the 2nd. It's more interesting, it has more Oomph, and it's nowhere near as sluggish as the 2nd. I wasn't a big fan of Ray Charles to begin with to be honest, so I really didn't have any expectations for the film, what so ever. Ray's biggest problem had to be the over length. This could've been cut very easily, with more relevant scenes, other then the ones they used. I found the early part of Ray's life when he was just starting to get successful, the most interesting. He was humble back then, and somewhat a gentleman. And while the film may have over exaggerated his actions, he got a bit too full of himself for me to care.<br /><br />Performance. Jamie Fox gives a performance for the ages as Ray. He looks like Ray, talks like Ray, acts like Ray. He even sings like Ray!. This is much more then an impression, I truly believed he WAS, Ray Charles. He was the heart of the film, and without his presence, this film would've been a complete and utter bore.<br /><br />Bottom Line. Ray is interesting at times, dreadfully dull at others. When all was said and done, I was disappointed by how routine it seemed at times. Just because it's a bibliographical film, Doesen't mean it's automatically Oscar worthy. Jamie Fox deserved his Oscar, but the movie is above average at best. Worth a watch, but if I were you, I'd keep my expectations at a rather comfortable level.<br /><br />7/10\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a Pseudo-random number generator (PRNG)\n",
    "rng = np.random.default_rng(420)\n",
    "\n",
    "wrong_predicted_data[int(rng.random() * len(wrong_predicted_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que ce commentaire est une critique constructive du film.\n",
    "Par exemple dans la phrase : \"Ray is interesting at times, dreadfully dull at others. When all was said and done, I was disappointed by how routine it seemed at times. Just because it's a bibliographical film, Doesen't mean it's automatically Oscar worthy.\", l'individu n'hésite pas à pointer du doigt ce qui ne lui a pas plu bien qu'appréciant le film.\n",
    "Il est donc compliqué de se baser sur une simple analyse syntaxique pour déterminer si c'est une revue positive ou négative quand l'individu est particulièrement critique de certains points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I have to hold Barney drilling my head every day; well.. I guess there must be reasons. First, I\\'m convinced that our kids are not stupids, they are just kids, but they know (my 1 and a half years old son \"selects\" what to see) what\\'s nice or disgusting. Did you see the news? Do you think your kids HAVE TO KNOW the reality as it is? Maybe..or maybe not; we (the adults) have the responsibility about what we want for our kids, and what to teach them. A film of drug dealers? news about massacres in Middle East? Of course, the kids must know there is a Real Life, but... they are kids; let\\'s give them some mercy. What do you want for them? If you wanna have kids trained on weapons or the best way to kill a neighbor, go ahead, impose them Lethal Weapon, Kill Bill, any manga\\'s anime, tell them Santa\\'s a depraved who enters through the chimney directly to violate them. I want illusions for my son (don\\'t get me wrong, I\\'m not saying Barney and Friends is the best; in fact, the show have a lot of defects, I read other comments and I agree with most); maybe the happiness is made of dreams, or illusions. At least, I want to teach him to grow WITHOUT FEAR BUT CAUTIOUS, that learns to think and believe that everything is not serial killers or hijackers, whom they\\'re reasons to worth to grow. That, at least, he can be a little happy with his own dreams. So, parents, don\\'t underestimate your kids; they know what they want.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predicted_data[int(rng.random() * len(wrong_predicted_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque dans cette critique un lexique de mots connotés négativement : \"disgusting\", \"drug dealers\", \"fear\", \"lethal\" ou encore \"violate\".\n",
    "De plus, cette critique est plus un avis sur ce que l'auteur pense de la société. Et même s'il a pu aimer le film, il n'en parle jamais directement. Il ne subsiste donc que très peu de mots ayant un registre positif (\"happy\" à la fin) pour une critique qui l'est.\n",
    "\n",
    "Nous pouvons ajouter que des tournures de phrases peu conventionnelles ou l'utilisation de l'ironie induit fortement en erreur ce type d'analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Bonus] What are the top 10 most important words (features) for each class? (bonus points)\n",
    "\n",
    "    * Look at the words with the highest likelihood in each class (if you use scikit-learn, you want to check feature_log_prob_).\n",
    "    * Remove stopwords (see NLTK stopwords corpus) and check again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les mots les plus significatifs avec les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['the', 'a', 'and', 'of', 'to', 'is', 'in', 'this', 'i', 'it'],\n",
       " 1: ['the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom.get_10_highest_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mots les plus significatifs correspondent à des stop words, ce sont des mots qui n'aident pas à trouver à quelle classe appartient le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processing des \"stop words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(comment: Dict[str, Any], stop_words: set = stops) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove every english stop words as defined in the nltk package\n",
    "    :param comment: Actual customer comment as a dict { str : Any }\n",
    "    :param stopWords: A set of stop words\n",
    "    :return: a modified dict { str : Any }\n",
    "    \"\"\"\n",
    "    words = word_tokenize(comment[\"text\"])\n",
    "    words_filtered = [w for w in words if w not in stop_words]\n",
    "    comment[\"text\"] = ' '.join(words_filtered)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "adapted_data_no_stop = adapted_data.map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- actors ? filmmakers ? certainly could n't audience- among air-puffed productions existence 's kind movie looks like lot fun shoot much fun nobody getting actual work done almost always makes movie 's fun watch ritter dons glasses hammer home character 's status sort doppleganger bespectacled bogdanovich scenes breezy ms stratten sweet embarrassing look-guys-i'm-dating-the-prom-queen feel ben gazzara sports usual cat's-got-canary grin futile attempt elevate meager plot requires pursue audrey hepburn interest narcoleptic insomnia clinic meantime budding couple 's respective children nepotism alert bogdanovich 's daughters spew cute pick fairly disturbing pointers 'love ' observing parents ms hepburn drawing dignity manages rise proceedings- monumental challenge playing ostensibly everybody looks great ? 's movie expect much 's 're looking 'd better picking copy vogue oh- mentioned colleen camp thoroughly annoys even apart singing competent wholly unconvincing country western numbers woefully mismatched standards soundtrack surely gershwin wrote song movie 's title derived mind stage musicals 20 's may slight least long charm laughed tries coast good intentions nobody- least peter bogdanovich - good sense put brakes due small part tragic death dorothy stratten movie special place heart mr bogdanovich- even bought back producers distributed went bankrupt n't prove popular rise fall among sympathetic tragic hollywood stories 's joy criticizing film real emotional investment ms stratten 's scenes laughed faint echo last picture show paper moon 's doc - following daisy miller long last love thundering confirmation phase p b never emerged though movie harmless waste rental want watch people good time 'll go park sunny day filmic expressions joy love 'll stick ernest lubitsch jaques demy\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapted_data_no_stop[\"train\"][\"text\"][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing this corpus with our Naives Bayes predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_2 = CustomNaivesBayesClassifier()\n",
    "custom_2.train_naive_bayes(adapted_data_no_stop[\"train\"][\"text\"], adapted_data_no_stop[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bayes_results_no_stop = custom_2.test_naive_bayes(adapted_data_no_stop[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82776"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(custom_bayes_results_no_stop, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce pré-processing des données permet bel est bien d'améliorer la qualité de prédiction de l'algorithme. Nous passons d'une prédiction correcte à 81.536% à 82.776%\n",
    "\n",
    "Donc un gain absolu de 1.24% et un gain relatif de 1.07%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with sky-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_no_stop = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_no_stop.fit(adapted_data_no_stop[\"train\"][\"text\"], adapted_data_no_stop[\"train\"][\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_bayes_no_stop = pipeline_no_stop.predict(adapted_data_no_stop[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82468"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(sklearn_bayes_no_stop, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois, nous améliorons la qualité de prédiction, on passe de 81.432% de bonne prédiction à 82.468% avec ce pré-processing.\n",
    "\n",
    "Un gain absolu de 1.036% et relatif de 1.06%.\n",
    "\n",
    "Cette différence doit être due au fait que sky-learn traite déjà les données pour réduire ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Bonus] Play with scikit-learn's version parameters. For example, see if you can consider unigram and bigram instead of only unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_bigram = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_bigram.fit(adapted_data_no_stop[\"train\"][\"text\"], adapted_data_no_stop[\"train\"][\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_bayes_bigram = pipeline_bigram.predict(adapted_data_no_stop[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85276"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(sklearn_bayes_bigram, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation de bigram améliore la qualité de prédiction, nous passons de 81.432% à 85.276% ce qui est une amélioration absolue de 3.844% et relative de 26%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of unigram in skikit-learning\n",
    "pipeline_unigram = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_unigram.fit(adapted_data_no_stop[\"train\"][\"text\"], adapted_data_no_stop[\"train\"][\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_bayes_unigram = pipeline_unigram.predict(adapted_data_no_stop[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82468"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(sklearn_bayes_unigram, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation d'unigram améliore également la qualité de prédiction, nous passons de 81.432% à 82.468% ce qui est une amélioration absolue de 0.936% et relative de 1.15%.\n",
    "\n",
    "La différence est cependant moins prononcée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les mots les plus significatifs sans les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [\"'s\", 'movie', \"n't\", 'film', 'one', '!', '?', 'like', 'would', 'even'],\n",
       " 1: [\"'s\",\n",
       "  'film',\n",
       "  'movie',\n",
       "  \"n't\",\n",
       "  'one',\n",
       "  '!',\n",
       "  'like',\n",
       "  'good',\n",
       "  'story',\n",
       "  'great']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_2.get_10_highest_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois-ci certains mots sont différents, dans le label positif on retrouve donc des mots élogieux comme 'good' ou 'great'.\n",
    "Cependant, pour le label négatif, à part le fait que \"n't\" soit en 3ᵉ position, nous ne retrouvons pas de mots connotés négativement dans le langage courant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2 points) Add stemming or lemmatization to your pretreatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va effectuer une stemmisation grâce à la bibliothèque NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(comment: Dict[str, Any], stemmer: PorterStemmer = ps) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Modify every word with the stemming implementation\n",
    "    :param [IN] stemmer:\n",
    "    :param comment: Actual customer comment as a dict { str : Any }\n",
    "    :return: a modified dict { str : Any }\n",
    "    \"\"\"\n",
    "    words = word_tokenize(comment[\"text\"])\n",
    "    words_stemmed = [stemmer.stem(w) for w in words]\n",
    "    comment[\"text\"] = ' '.join(words_stemmed)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) Train and evaluate your model again with these pretreatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "adapted_data_stemmed = adapted_data_no_stop.map(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de la stemmisation sur notre implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3 = CustomNaivesBayesClassifier()\n",
    "custom_3.train_naive_bayes(adapted_data_stemmed[\"train\"][\"text\"], adapted_data_stemmed[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bayes_results_stemming = custom_3.test_naive_bayes(adapted_data_stemmed[\"test\"][\"text\"])\n",
    "len(custom_bayes_results_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81956"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(custom_bayes_results_stemming, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) Are the results better or worse? Try explaining why the accuracy changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons 0.81956% de réussite, contre une précision de 0.82776% précédemment.\n",
    "\n",
    "Nous pouvons expliquer ce résultat par le fait que la langue anglaise n'est pas une langue qui se construit sur la suffixation de racines contrairement à des langues comme l'allemand ou le turc.\n",
    "\n",
    "Ainsi, réduire le lexique pourrait mener à perdre en précision quant aux sens des différents mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de la stemmisation sur sk-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stem = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_stem.fit(adapted_data_stemmed[\"train\"][\"text\"], adapted_data_stemmed[\"train\"][\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_bayes_stem = pipeline_stem.predict(adapted_data_stemmed[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81744"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bayes(sklearn_bayes_stem, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons 0.81744% de réussite, contre une précision de 0.82468% précédemment.\n",
    "\n",
    "Même commentaire sur l'impact de la stemmisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebdf6852bbc85c57be6fc9526280e045fa5a9d6bf2f96c878c409a53849ccadf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
