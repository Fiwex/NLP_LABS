{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 - Lab 06\n",
    "## Adrien Giget, Tanguy Malandain, Denis Stojiljkovic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sacrebleu.metrics import BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer training\n",
    "\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in tqdm(range(1, NUM_EPOCHS+1)):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour gagner du temps, nous avons sauvegarder le modèle dans le fichier `transformer_18_epochs.pt` pour ne pas avoir à relancer à chaque fois l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformer to not rerun the training every time\n",
    "# torch.save(transformer.state_dict(), 'transformer_18_epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved transformer\n",
    "# transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "#                                  NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# transformer = transformer.to(DEVICE)\n",
    "# transformer.load_state_dict(torch.load('transformer_18_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str, decode_function, **decode_args):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = decode_function(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, **decode_args).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of people stand in front of an igloo \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", greedy_decode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on traduit avec Google Translate, on a: \"A group of people stand in front of an igloo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4 points) Theoretical questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the positional encoding, why are we using a combination of sinus and cosinus?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les encodages positionnels sont utilisés pour donner une idée de l'ordre des mots, étant donné que le mécanisme d'auto-attention du transformers ne comprend pas intrinsèquement les informations positionnelles ou séquentielles. Ces codages sont ajoutés aux enchâssements de mots avant leur entrée dans le transformer.\n",
    "\n",
    "Les fonctions sinus et cosinus sont utilisées parce qu'elles permettent d'encoder la position avec une combinaison unique et réversible de valeurs qui peuvent s'adapter facilement à différentes longueurs de séquences. Chaque dimension du codage positionnel correspond à une fonction sinus ou cosinus avec une fréquence différente, ce qui permet au modèle d'apprendre à assister aux positions relatives puisque pour tout décalage fixe k, `Pos + k` peut être représenté comme une fonction linéaire de Pos.\n",
    "\n",
    "Ainsi, l'utilisation de ces fonctions permet au modèle d'extrapoler à des longueurs de séquence en dehors de l'ensemble d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the Seq2SeqTransformer class:\n",
    "- What is the parameter nhead for?\n",
    "- What is the point of the generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre nhead de la classe Seq2SeqTransformer fait référence au nombre de têtes dans le mécanisme d'auto-attention multi-têtes. Dans ce mécanisme, le modèle crée plusieurs sous-espaces (ou \"têtes\") des données d'entrée et applique à chacun d'eux un dot-product mis à l'échelle. Les résultats sont ensuite concaténés et transformés linéairement pour obtenir la sortie finale. L'idée est que différentes têtes peuvent apprendre à se concentrer sur différents types d'informations dans les données d'entrée, ce qui rend le modèle plus puissant et plus flexible. Le paramètre nhead détermine le nombre de ces sous-espaces.\n",
    "\n",
    "Dans le contexte d'un modèle de transformer, un générateur est généralement une couche linéaire qui fait correspondre la sortie du transformer (qui est dans la dimension de l'état caché) à la taille du vocabulaire. L'objectif est de générer les jetons de sortie finaux à partir de la représentation interne du modèle. Dans de nombreux cas, après le générateur, une fonction softmax est appliquée pour obtenir une distribution de probabilité sur tous les mots de sortie possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the goal of the create_mask function. Why does it handle differently the source and target masks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `create_mask` génère des masques pour un modèle séquence à séquence, en particulier pour les séquences source (`src`) et cible (`tgt`).\n",
    "\n",
    "1. Le masque `src_mask` permet au modèle d'accéder à tous les tokens de la source car la séquence d'entrée entière est généralement accessible pendant le décodage.\n",
    "2. `tgt_mask` empêche le \"look-ahead\", en interdisant l'accès aux futurs tokens cibles pendant la prédiction en cours.\n",
    "3. `src_padding_mask` et `tgt_padding_mask` ignorent les jetons de remplissage ajoutés pour normaliser la longueur des séquences.\n",
    "\n",
    "La différence de traitement est due à la nature séquentielle de la tâche. Pour les sources, tous les tokens sont accessibles en même temps ; pour les cibles, le modèle doit prédire tokens par tokens sans voir les tokens futurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Decoding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial uses a greedy approach at decoding. Implement the following variations.\n",
    "* (3 points) A top-k sampling with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling_decode(model, src, src_mask, max_len, start_symbol, temperature=1.0, top_k=10):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        logits = model.generator(out[:, -1])\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Extract top-k tokens\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "        \n",
    "        # Sample from top-k probabilities\n",
    "        next_word = torch.multinomial(top_k_probs, num_samples=1)\n",
    "        next_word_idx = top_k_indices.gather(dim=1, index=next_word)\n",
    "        next_word_idx = next_word_idx.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word_idx)], dim=0)\n",
    "        if next_word_idx == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of people stand in front of an igloo \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu.\", top_k_sampling_decode, temperature=0.8, top_k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 point) A top-p sampling with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling_decode(model, src, src_mask, max_len, start_symbol, temperature=1.0, top_p=0.9):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        logits = model.generator(out[:, -1])\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sort probabilities and corresponding indices\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "        # Compute cumulative probabilities\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Create mask for probabilities that exceed the threshold\n",
    "        mask = cum_probs > top_p\n",
    "\n",
    "        # If at least one token was masked, ignore the first one (it has the highest probability)\n",
    "        if mask.sum() > 0:\n",
    "            if mask.shape[0] > 1:\n",
    "                mask = torch.cat((torch.tensor([False]).to(DEVICE), mask[:-1]))\n",
    "            else:\n",
    "                mask = torch.tensor([False]).to(DEVICE)\n",
    "        \n",
    "        # Apply the mask to sorted probabilities and indices\n",
    "        filtered_probs = sorted_probs.masked_fill(mask, 0.0)\n",
    "        filtered_indices = sorted_indices.masked_fill(mask, 0)\n",
    "\n",
    "        # Sample from the filtered probabilities\n",
    "        next_word_idx = torch.multinomial(filtered_probs, num_samples=1)\n",
    "        next_word_idx = filtered_indices.gather(dim=1, index=next_word_idx)\n",
    "        next_word_idx = next_word_idx.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word_idx)], dim=0)\n",
    "        if next_word_idx == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of people standing in front of an igloo \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", top_p_sampling_decode, temperature=0.4, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2 point) Play with the k, p and temperature parameters, and qualitatively compare a few (at least 3) translation samples for each approach (even the greedy one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va d'abord récupérer 3 sample à traduire ainsi que leur target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "\n",
    "i = 0\n",
    "phrases, targets = [], []\n",
    "\n",
    "for src, tgt in val_iter:\n",
    "    phrases.append(src)\n",
    "    targets.append(tgt)\n",
    "    \n",
    "    i += 1\n",
    "    if i == 3:\n",
    "        break\n",
    "        \n",
    "phrase1, target1 = phrases[0], targets[0]\n",
    "phrase2, target2 = phrases[1], targets[1]\n",
    "phrase3, target3 = phrases[2], targets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premer sample: Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "Target: A group of men are loading cotton onto a truck\n"
     ]
    }
   ],
   "source": [
    "print(f\"Premer sample: {phrase1}\\nTarget: {target1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deuxième sample: Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "Target: A man sleeping in a green room on a couch.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Deuxième sample: {phrase2}\\nTarget: {target2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Troisième sample: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
      "Target: A boy wearing headphones sits on a woman's shoulders.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Troisième sample: {phrase3}\\nTarget: {target3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men loading animal cotton into a truck .  Target: A group of men are loading cotton onto a truck\n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, greedy_decode), \"Target:\", target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is sleeping on a couch in a green room .  Target: A man sleeping in a green room on a couch.\n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, greedy_decode), \"Target:\", target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A boy with headphones sitting on his shoulders while sitting on a woman .  Target: A boy wearing headphones sits on a woman's shoulders.\n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, greedy_decode), \"Target:\", target3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les traductions générées par le modèle semblent être globalement correctes, mais elles ne sont pas parfaites. Elles parviennent à transmettre le sens général de chaque phrase, mais il y a des erreurs spécifiques dans chaque exemple.\n",
    "\n",
    "\"A group of men loading animal cotton into a truck.\" Au lieu de \"animal cotton\", le terme correct serait simplement \"cotton\". Il semble que le modèle ait mal traduit ou ajouté des informations supplémentaires qui n'étaient pas dans la phrase d'origine.\n",
    "\n",
    "\"A man is sleeping on a couch in a green room.\" Cette traduction semble correcte.\n",
    "\n",
    "\"A boy with headphones sitting on his shoulders while sitting on a woman.\" Ici, le modèle a mal interprété la relation spatiale entre le garçon et la femme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top_k Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase 1: Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men loading cotton into a truck . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, top_k_sampling_decode, temperature=0.8, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men unloading onto a truck . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, top_k_sampling_decode, temperature=0.8, top_k=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men loading animal cotton over a truck . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, top_k_sampling_decode, temperature=0.4, top_k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le top_k_sampling_decode, le résultat ne sera pas forcément le même, on va donc prendre en compte une simulation que l'on a eu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target : \"A group of men are loading cotton onto a truck\"**\n",
    "\n",
    "**Température=0.8, top_k=10** : \"A group of men loading cotton into a truck.\"<br>Cette traduction est correcte et assez proche de la phrase cible.\n",
    "\n",
    "**Température=0.8, top_k=15** : \"A group of men unloading onto a truck .\"<br>Cette traduction a une erreur majeure : \"unloading\" au lieu de \"loading\". De plus, le \"cotton\" a été omis.\n",
    "\n",
    "**Température=0.4, top_k=10** : \"A group of men loading animal cotton over a truck .\"<br>Ici aussi, nous avons une erreur où \"animal cotton\" est incorrect et \"over\" n'est pas la bonne préposition dans ce contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase 2: Ein Mann schläft in einem grünen Raum auf einem Sofa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man sleeping in a green room on a couch . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, top_k_sampling_decode, temperature=0.8, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is asleep on a couch in a green room . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, top_k_sampling_decode, temperature=0.8, top_k=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is sleeping in a green room on a couch . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, top_k_sampling_decode, temperature=0.4, top_k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target : \"A man sleeping in a green room on a couch.\"**\n",
    "\n",
    "Les trois traductions pour cette phrase sont toutes assez correctes, bien que certaines varient légèrement en termes de structure grammaticale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase 3: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A boy wearing headphones is sitting on his shoulders by a woman . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, top_k_sampling_decode, temperature=0.8, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A young boy with headphones on sitting on top of a woman . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, top_k_sampling_decode, temperature=0.8, top_k=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A boy with headphones sitting on his shoulders while sitting on a woman . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, top_k_sampling_decode, temperature=0.4, top_k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target \"A boy wearing headphones sits on a woman's shoulders.\"**\n",
    "\n",
    "**Température=0.8, top_k=10** : \"A boy with headphones sitting on his shoulders with a woman on her shoulders .\"<br>Cette traduction est incorrecte car le garçon est censé être assis sur les épaules de la femme, pas les siennes.\n",
    "\n",
    "**Température=0.8, top_k=15** : \"A young boy wearing headphones is sitting on her shoulders .\"<br>Cette phrase est partiellement correcte, mais le pronom \"her\" est ambigu sans contexte supplémentaire.\n",
    "\n",
    "**Température=0.4, top_k=10** : \"A boy wearing headphones is sitting on his shoulders with a woman .\"<br>Ici aussi, la phrase suggère de façon incorrecte que le garçon est assis sur ses propres épaules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les traductions générées par le modèle en utilisant le top_k_sampling_decode présentent des erreurs et des variations intéressantes. L'utilisation de la méthode de top_k_sampling_decode permet d'introduire plus de diversité dans les traductions générées par le modèle. Cependant, cela peut aussi introduire des erreurs ou des phrases qui n'ont pas de sens. Une valeur plus élevée du top_k conduit à plus de diversité mais peut aussi conduire à plus d'erreurs et une valeur plus élevée de la température donne une traduction qui varie plus entre les différentes tentatives de traduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top_p Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase 1: Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men putting pot on a truck into a truck . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, top_p_sampling_decode, temperature=0.8, top_p=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men putting out boxes on a truck . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, top_p_sampling_decode, temperature=0.8, top_p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of men loading animal cotton into a truck . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase1, top_p_sampling_decode, temperature=0.4, top_p=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le top_k_sampling_decode, le résultat ne sera pas forcément le même, on va donc prendre en compte une simulation que l'on a eu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target : \"A group of men are loading cotton onto a truck\"**\n",
    "\n",
    "**Température=0.8, top_k=0.8** : \"A group of men putting pot on a truck into a truck .\"<br>La phrase est mal traduite, les mots 'putting pot' ne correspondent pas à l'original et il y a une répétition.\n",
    "\n",
    "**Température=0.8, top_k=0.5** : \"A group of men putting out boxes on a truck .\"<br>La traduction est incorrecte, 'putting out boxes on a truck' ne contient pas l'information 'cotton' et 'on' n'est pas la bonne préposition.\n",
    "\n",
    "**Température=0.4, top_k=0.8** : \"A group of men loading animal cotton into a truck .\"<br>La phrase est correcte mais on a l'ajout de 'animal' qui est en trop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase 2: Ein Mann schläft in einem grünen Raum auf einem Sofa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is sleeping in a green room on a sofa . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, top_p_sampling_decode, temperature=0.8, top_p=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is sleeping while on a couch in a green room . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, top_p_sampling_decode, temperature=0.8, top_p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is sleeping on a couch in a green room . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase2, top_p_sampling_decode, temperature=0.4, top_p=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target : \"A man sleeping in a green room on a couch.\"**\n",
    "\n",
    "Les trois traductions pour cette phrase sont toutes correctes, même si pour la première 'couch' a été remplacé par 'sofa'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase 3: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A boy wearing sunglasses is sitting on a woman 's shoulders . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, top_p_sampling_decode, temperature=0.8, top_p=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A boy wearing headphones is sitting on a woman 's shoulders . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, top_p_sampling_decode, temperature=0.8, top_p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A boy wearing headphones is sitting on his shoulders while sitting on a woman . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, phrase3, top_p_sampling_decode, temperature=0.4, top_p=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target \"A boy wearing headphones sits on a woman's shoulders.\"**\n",
    "\n",
    "**Température=0.8, top_k=0.8** : \"A boy wearing sunglasses is sitting on a woman 's shoulders .\"<br>Une traduction assez correcte mais qui a cependant remplacé 'headphones' par 'sunglasses'.\n",
    "\n",
    "**Température=0.8, top_k=0.5** : \"A boy wearing headphones is sitting on a woman 's shoulders .\"<br>La traduction est correcte.\n",
    "\n",
    "**Température=0.4, top_k=0.8** : \"A boy wearing headphones is sitting on his shoulders while sitting on a woman .\"<br>Ici 'shoulders' a été associé au garçon et non pas à la femme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même façon que le top_k_sampling_decode, les résultats ne sont pas toujours précis et dépendent fortement des valeurs de température et de top_p. Le top_p contrôle la diversité des prédictions. Un top_p plus faible signifie que seuls les tokens les plus probables sont conservés, tandis qu'un top_p plus élevé permet de conserver plus de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(2 points)** Compute the BLEU score of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [sacreBLEU](https://github.com/mjpost/sacreBLEU) implementation to evaluate your model and quantitatively compare the 3 implemented decoding approaches on the test set. Explain what all the output values mean (when using the `corpus_score` function).\n",
    "\n",
    "In the [python section](https://github.com/mjpost/sacrebleu#using-sacrebleu-from-python), you'll notice the library accepts more than just one possible translation as reference, but the given dataset only has one translation per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, decode_function, **decode_args):\n",
    "    references = []\n",
    "    candidates = []\n",
    "    \n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    \n",
    "    for src_sentence, tgt_sentence in val_iter:\n",
    "        translation = translate(model, src_sentence, decode_function, **decode_args)\n",
    "        candidates.append(translation)\n",
    "        references.append([tgt_sentence])\n",
    "        \n",
    "    bleu = BLEU()\n",
    "    \n",
    "    score = bleu.corpus_score(candidates, references)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decode BLEU: BLEU = 35.93 100.0/60.0/22.2/12.5 (BP = 1.000 ratio = 1.000 hyp_len = 11 ref_len = 11)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with greedy decode\n",
    "greedy_bleu = evaluate_model(transformer, greedy_decode)\n",
    "print('Greedy Decode BLEU:', greedy_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expliquons les outputs de la fonction `corpus_score` sur le greedy_decode:\n",
    "\n",
    "- BLEU = 35.93 : c'est le score BLEU pour le texte évalué qui mesure la qualité de la sortie de la traduction automatique par rapport aux traductions de référence.\n",
    "\n",
    "- 100.0/60.0/22.2/12.5 : Ce sont les scores de précision pour les correspondances 1-gramme, 2-grammes, 3-grammes et 4-grammes respectivement. Les scores mesurent combien des n-grammes dans les traductions générées par la machine correspondent aux n-grammes dans les traductions de référence.\n",
    "\n",
    "- BP = 1.000 : Il s'agit de la Pénalité de Brèveté. La Pénalité de Brèveté pénalise les traductions automatiques qui sont plus courtes que les traductions de référence, car les traductions plus courtes omettent souvent des informations importantes. La Pénalité de Brèveté est un nombre compris entre 0 et 1. Si la traduction automatique est de la même longueur ou plus longue que la référence, la Pénalité de Brèveté est 1 et n'a aucun effet.\n",
    "\n",
    "- ratio = 1.000 : Il s'agit du rapport de longueur entre la traduction automatique et la traduction de référence.\n",
    "\n",
    "- hyp_len = 11 : C'est la longueur de l'hypothèse (traduction automatique) en tokens.\n",
    "\n",
    "- ref_len = 11 : C'est la longueur de référence effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k Decode BLEU: BLEU = 40.82 100.0/77.8/25.0/14.3 (BP = 1.000 ratio = 1.000 hyp_len = 10 ref_len = 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with top-k sampling decode\n",
    "top_k_bleu = evaluate_model(transformer, top_k_sampling_decode, temperature=0.8, top_k=10)\n",
    "print('Top-k Decode BLEU:', top_k_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p Decode BLEU: BLEU = 35.08 90.9/60.0/22.2/12.5 (BP = 1.000 ratio = 1.000 hyp_len = 11 ref_len = 11)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with top-p sampling decode\n",
    "top_p_bleu = evaluate_model(transformer, top_p_sampling_decode, temperature=0.8, top_p=0.8)\n",
    "print('Top-p Decode BLEU:', top_p_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas, le top_k_sampling_decode semble offrir la meilleure performance avec un score BLEU de 40.82. Elle présente également une précision supérieure pour les 2-grammes, 3-grammes et 4-grammes par rapport aux autres approches. Cependant, pour être certain de ce résultat, il faudrait faire varier les hyperparamètres temperature, top_k et top_p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `translate` function provided in the tutorial is pretty slow, as it translate text by text. It's recommended you modify the function to accept a list of texts as input, and batch them for translations (also **bonus point**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentences: list, decode_function, **decode_args):\n",
    "    model.eval()\n",
    "    translated_sentences = []\n",
    "    for src_sentence in src_sentences:\n",
    "        src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "        num_tokens = src.shape[0]\n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "        tgt_tokens = decode_function(\n",
    "            model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, **decode_args).flatten()\n",
    "        translated_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy())))\n",
    "        translated_sentence = translated_sentence.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "        translated_sentences.append(translated_sentence)\n",
    "    return translated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Bonus\\]** Use part of the test set to perform an hyperparameters search on the value of temperature, k, and p. Note that, normally, this should be done on a validation set, not the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, nous n'avons pas réussi à batcher les phrases avant de les passer dans la fonction translate, par conséquent nous n'avons pas eu le temps de réaliser la recherche des hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_values = [0.5, 0.7, 0.9, 1.0, 1.2]\n",
    "top_k_values = [5, 10, 20, 40, 50]\n",
    "\n",
    "best_score = 0.0\n",
    "best_params = None\n",
    "\n",
    "for temperature in tqdm(temperature_values):\n",
    "    for top_k in top_k_values:\n",
    "        # Compute BLEU score\n",
    "        current_score = evaluate_model(transformer, top_k_sampling_decode, temperature=temperature, top_k=top_k).score\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = (temperature, top_k)\n",
    "            \n",
    "print(f\"Best score: {best_score}\")\n",
    "print(f\"Best parameters: Temperature={best_params[0]}, Top-k={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_values = [0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "best_score = 0.0\n",
    "best_params = None\n",
    "\n",
    "for temperature in tqdm(temperature_values):\n",
    "    for top_p in top_p_values:\n",
    "        # Compute BLEU score\n",
    "        current_score = evaluate_model(transformer, top_p_sampling_decode, temperature=temperature, top_p=top_p).score\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = (temperature, top_p)\n",
    "                \n",
    "print(f\"Best score: {best_score}\")\n",
    "print(f\"Best parameters: Temperature={best_params[0]}, Top-p={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
