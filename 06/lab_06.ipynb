{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP 2 - Lab 06\n",
    "## Adrien Giget, Tanguy Malandain, Denis Stojiljkovic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## (4 points) Theoretical questions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In the positional encoding, why are we using a combination of sinus and cosinus?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Les encodages positionnels sont utilisés pour donner une idée de l'ordre des mots, étant donné que le mécanisme d'auto-attention du transformers ne comprend pas intrinsèquement les informations positionnelles ou séquentielles. Ces codages sont ajoutés aux enchâssements de mots avant leur entrée dans le transformer.\n",
    "\n",
    "Les fonctions sinus et cosinus sont utilisées parce qu'elles permettent d'encoder la position avec une combinaison unique et réversible de valeurs qui peuvent s'adapter facilement à différentes longueurs de séquences. Chaque dimension du codage positionnel correspond à une fonction sinus ou cosinus avec une fréquence différente, ce qui permet au modèle d'apprendre à assister aux positions relatives puisque pour tout décalage fixe k, `Pos + k` peut être représenté comme une fonction linéaire de Pos.\n",
    "\n",
    "Ainsi, l'utilisation de ces fonctions permet au modèle d'extrapoler à des longueurs de séquence en dehors de l'ensemble d'apprentissage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In the Seq2SeqTransformer class:\n",
    "- What is the parameter nhead for?\n",
    "- What is the point of the generator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Le paramètre nhead de la classe Seq2SeqTransformer fait référence au nombre de têtes dans le mécanisme d'auto-attention multi-têtes. Dans ce mécanisme, le modèle crée plusieurs sous-espaces (ou \"têtes\") des données d'entrée et applique à chacun d'eux un dot-product mis à l'échelle. Les résultats sont ensuite concaténés et transformés linéairement pour obtenir la sortie finale. L'idée est que différentes têtes peuvent apprendre à se concentrer sur différents types d'informations dans les données d'entrée, ce qui rend le modèle plus puissant et plus flexible. Le paramètre nhead détermine le nombre de ces sous-espaces.\n",
    "\n",
    "Dans le contexte d'un modèle de transformer, un générateur est généralement une couche linéaire qui fait correspondre la sortie du transformer (qui est dans la dimension de l'état caché) à la taille du vocabulaire. L'objectif est de générer les jetons de sortie finaux à partir de la représentation interne du modèle. Dans de nombreux cas, après le générateur, une fonction softmax est appliquée pour obtenir une distribution de probabilité sur tous les mots de sortie possibles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Describe the goal of the create_mask function. Why does it handle differently the source and target masks?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "La fonction `create_mask` génère des masques pour un modèle séquence à séquence, en particulier pour les séquences source (`src`) et cible (`tgt`).\n",
    "\n",
    "1. Le masque `src_mask` permet au modèle d'accéder à tous les tokens de la source car la séquence d'entrée entière est généralement accessible pendant le décodage.\n",
    "2. `tgt_mask` empêche le \"look-ahead\", en interdisant l'accès aux futurs tokens cibles pendant la prédiction en cours.\n",
    "3. `src_padding_mask` et `tgt_padding_mask` ignorent les jetons de remplissage ajoutés pour normaliser la longueur des séquences.\n",
    "\n",
    "La différence de traitement est due à la nature séquentielle de la tâche. Pour les sources, tous les tokens sont accessibles en même temps ; pour les cibles, le modèle doit prédire tokens par tokens sans voir les tokens futurs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## (6 points) Decoding functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
